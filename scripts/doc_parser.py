#!/usr/bin/env python3
"""
Documentation Parser for Auto Documentation Completion
Parses existing documentation to identify documented vs missing code files.
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple

class DocumentationParser:
    def __init__(self, repo_root: str):
        self.repo_root = repo_root
        self.doc_files = [
            'docs/æ•™è‚²AIé¢˜åº“ç³»ç»Ÿæ–‡æ¡£.md',
            'docs/æ•™è‚²AIå…¨æ ˆè®¾è®¡æ–¹æ¡ˆ.md', 
            'docs/ç³»ç»ŸUIè®¾è®¡æ–¹æ¡ˆæ–‡æ¡£.md'
        ]
        
        # åŒ¹é…ä»£ç å—çš„æ­£åˆ™æ¨¡å¼
        self.code_block_patterns = [
            r'####\s*([^\n]+)\n```(\w+)\n(.*?)```',  # #### filename
            r'###\s*([^\n]+)\n```(\w+)\n(.*?)```',   # ### filename
            r'##\s*([^\n]+)\n```(\w+)\n(.*?)```',    # ## filename
            r'æ–‡ä»¶å[:ï¼š]\s*([^\n]+)\n```(\w+)\n(.*?)```',  # æ–‡ä»¶å: filename
            r'[#]*\s*([^\n]*\.(?:py|js|vue|yaml|yml|json|md|sql|sh))\s*\n```(\w+)\n(.*?)```'  # æ–‡ä»¶æ‰©å±•å
        ]
        
        # è·¯å¾„å¼•ç”¨æ¨¡å¼
        self.path_reference_patterns = [
            r'(?:backend|frontend|k8s|scripts|docs)/[^\s\)]+\.(?:py|js|vue|yaml|yml|json|md|sql|sh)',
            r'[^\s\)]+/[^\s\)]+\.(?:py|js|vue|yaml|yml|json|md|sql|sh)'
        ]

    def parse_documentation(self) -> Dict:
        """è§£ææ–‡æ¡£ï¼Œæå–æ‰€æœ‰ä»£ç å¼•ç”¨å’Œä»£ç å—"""
        result = {
            'documented_files': {},  # æ–‡ä»¶è·¯å¾„ -> æ–‡æ¡£ä¿¡æ¯
            'code_blocks': [],       # ä»£ç å—åˆ—è¡¨
            'file_references': set(), # æ–‡ä»¶è·¯å¾„å¼•ç”¨
            'missing_files': set(),   # æ–‡æ¡£ä¸­å¼•ç”¨ä½†ä¸å­˜åœ¨çš„æ–‡ä»¶
            'doc_summary': {}         # æ¯ä¸ªæ–‡æ¡£çš„æ‘˜è¦
        }
        
        for doc_file in self.doc_files:
            doc_path = os.path.join(self.repo_root, doc_file)
            if not os.path.exists(doc_path):
                continue
                
            print(f"ğŸ“– è§£ææ–‡æ¡£: {doc_file}")
            
            with open(doc_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æä»£ç å—
            code_blocks = self._extract_code_blocks(content, doc_file)
            result['code_blocks'].extend(code_blocks)
            
            # è§£ææ–‡ä»¶è·¯å¾„å¼•ç”¨
            file_refs = self._extract_file_references(content)
            result['file_references'].update(file_refs)
            
            # æ–‡æ¡£æ‘˜è¦
            result['doc_summary'][doc_file] = {
                'code_blocks': len(code_blocks),
                'file_references': len(file_refs),
                'size': len(content)
            }
        
        # å¤„ç†æ–‡æ¡£åŒ–çš„æ–‡ä»¶
        for block in result['code_blocks']:
            if block['filename']:
                result['documented_files'][block['filename']] = {
                    'source_doc': block['source_doc'],
                    'language': block['language'],
                    'has_code': bool(block['code'].strip()),
                    'code_length': len(block['code'])
                }
        
        return result

    def _extract_code_blocks(self, content: str, source_doc: str) -> List[Dict]:
        """ä»å†…å®¹ä¸­æå–ä»£ç å—"""
        code_blocks = []
        
        for pattern in self.code_block_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for match in matches:
                if len(match) == 3:
                    filename, language, code = match
                else:
                    continue
                    
                # æ¸…ç†æ–‡ä»¶å
                filename = self._clean_filename(filename)
                
                if filename:
                    code_blocks.append({
                        'filename': filename,
                        'language': language.lower(),
                        'code': code.strip(),
                        'source_doc': source_doc
                    })
        
        return code_blocks

    def _extract_file_references(self, content: str) -> Set[str]:
        """ä»å†…å®¹ä¸­æå–æ–‡ä»¶è·¯å¾„å¼•ç”¨"""
        file_refs = set()
        
        for pattern in self.path_reference_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                cleaned_path = self._clean_path_reference(match)
                if cleaned_path:
                    file_refs.add(cleaned_path)
        
        return file_refs

    def _clean_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        # ç§»é™¤å‰å¯¼çš„ # ç¬¦å·å’Œç©ºæ ¼
        filename = re.sub(r'^[#\s]+', '', filename.strip())
        
        # ç§»é™¤ "æ–‡ä»¶å:" å‰ç¼€
        filename = re.sub(r'^æ–‡ä»¶å[:ï¼š]\s*', '', filename)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä½†ä¿ç•™è·¯å¾„åˆ†éš”ç¬¦ï¼‰
        filename = re.sub(r'[^\w\-_./\\]', '', filename)
        
        # æ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦
        filename = filename.replace('\\', '/')
        
        # ç§»é™¤å¤šä½™çš„è·¯å¾„åˆ†éš”ç¬¦
        filename = re.sub(r'/+', '/', filename)
        
        return filename if filename else None

    def _clean_path_reference(self, path: str) -> str:
        """æ¸…ç†è·¯å¾„å¼•ç”¨"""
        # ç§»é™¤å‘¨å›´çš„æ ‡ç‚¹ç¬¦å·
        path = re.sub(r'^[^\w/]+|[^\w/]+$', '', path)
        
        # æ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦
        path = path.replace('\\', '/')
        
        return path if path else None

    def compare_with_actual_files(self, inventory: Dict) -> Dict:
        """æ¯”è¾ƒæ–‡æ¡£åŒ–çš„æ–‡ä»¶ä¸å®é™…å­˜åœ¨çš„æ–‡ä»¶"""
        doc_result = self.parse_documentation()
        
        actual_files = {f['path'] for f in inventory['files']}
        documented_files = set(doc_result['documented_files'].keys())
        all_referenced_files = documented_files | doc_result['file_references']
        
        comparison = {
            'existing_but_undocumented': actual_files - all_referenced_files,
            'documented_but_missing': all_referenced_files - actual_files,
            'documented_and_existing': actual_files & documented_files,
            'only_referenced': doc_result['file_references'] - documented_files,
            'statistics': {
                'total_actual_files': len(actual_files),
                'total_documented_files': len(documented_files),
                'total_referenced_files': len(doc_result['file_references']),
                'undocumented_count': len(actual_files - all_referenced_files),
                'missing_count': len(all_referenced_files - actual_files)
            }
        }
        
        return comparison, doc_result

def main():
    """ä¸»å‡½æ•°"""
    repo_root = '/home/runner/work/Edu-ai-question-bank/Edu-ai-question-bank'
    
    # åŠ è½½ä»“åº“æ‰«æç»“æœ
    inventory_file = os.path.join(repo_root, 'scripts', 'repo_inventory.json')
    if not os.path.exists(inventory_file):
        print("âŒ è¯·å…ˆè¿è¡Œ repo_scanner.py ç”Ÿæˆæ–‡ä»¶æ¸…å•")
        return
    
    with open(inventory_file, 'r', encoding='utf-8') as f:
        inventory = json.load(f)
    
    # è§£ææ–‡æ¡£
    parser = DocumentationParser(repo_root)
    comparison, doc_result = parser.compare_with_actual_files(inventory)
    
    print("ğŸ“š æ–‡æ¡£è§£æå®Œæˆ\n")
    
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    stats = comparison['statistics']
    print(f"  å®é™…æ–‡ä»¶æ•°: {stats['total_actual_files']}")
    print(f"  æ–‡æ¡£åŒ–æ–‡ä»¶æ•°: {stats['total_documented_files']}")
    print(f"  å¼•ç”¨æ–‡ä»¶æ•°: {stats['total_referenced_files']}")
    print(f"  æœªæ–‡æ¡£åŒ–æ–‡ä»¶æ•°: {stats['undocumented_count']}")
    print(f"  ç¼ºå¤±æ–‡ä»¶æ•°: {stats['missing_count']}")
    
    print(f"\nğŸ“ æ–‡æ¡£æ‘˜è¦:")
    for doc, summary in doc_result['doc_summary'].items():
        print(f"  {doc}:")
        print(f"    ä»£ç å—: {summary['code_blocks']} ä¸ª")
        print(f"    æ–‡ä»¶å¼•ç”¨: {summary['file_references']} ä¸ª")
    
    print(f"\nğŸ†• å­˜åœ¨ä½†æœªæ–‡æ¡£åŒ–çš„æ–‡ä»¶ ({len(comparison['existing_but_undocumented'])}):")
    for file_path in sorted(comparison['existing_but_undocumented']):
        print(f"  - {file_path}")
    
    print(f"\nâŒ æ–‡æ¡£åŒ–ä½†ç¼ºå¤±çš„æ–‡ä»¶ ({len(comparison['documented_but_missing'])}):")
    for file_path in sorted(comparison['documented_but_missing']):
        print(f"  - {file_path}")
    
    print(f"\nâœ… æ–‡æ¡£åŒ–ä¸”å­˜åœ¨çš„æ–‡ä»¶ ({len(comparison['documented_and_existing'])}):")
    for file_path in sorted(comparison['documented_and_existing']):
        print(f"  - {file_path}")
    
    # ä¿å­˜åˆ†æç»“æœ
    output_file = os.path.join(repo_root, 'scripts', 'doc_analysis.json')
    analysis_result = {
        'comparison': comparison,
        'documentation_result': doc_result
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2, default=list)
    
    print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return analysis_result

if __name__ == "__main__":
    main()