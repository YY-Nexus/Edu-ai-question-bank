#!/usr/bin/env python3
"""
Auto Documentation Completion Engine
Automatically adds missing code files to documentation with descriptions and code blocks.
"""

import os
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime

class AutoDocumentationEngine:
    def __init__(self, repo_root: str):
        self.repo_root = repo_root
        self.main_docs = {
            'docs/æ•™è‚²AIé¢˜åº“ç³»ç»Ÿæ–‡æ¡£.md': 'system',
            'docs/æ•™è‚²AIå…¨æ ˆè®¾è®¡æ–¹æ¡ˆ.md': 'design', 
            'docs/ç³»ç»ŸUIè®¾è®¡æ–¹æ¡ˆæ–‡æ¡£.md': 'ui'
        }
        
        # æ–‡ä»¶åˆ†ç±»åˆ°æ–‡æ¡£çš„æ˜ å°„
        self.file_to_doc_mapping = {
            'backend': 'docs/æ•™è‚²AIå…¨æ ˆè®¾è®¡æ–¹æ¡ˆ.md',
            'frontend': 'docs/ç³»ç»ŸUIè®¾è®¡æ–¹æ¡ˆæ–‡æ¡£.md',
            'infrastructure': 'docs/æ•™è‚²AIé¢˜åº“ç³»ç»Ÿæ–‡æ¡£.md',
            'scripts': 'docs/æ•™è‚²AIé¢˜åº“ç³»ç»Ÿæ–‡æ¡£.md',
            'docs': None,  # ä¸è‡ªåŠ¨æ·»åŠ æ–‡æ¡£æ–‡ä»¶åˆ°æ–‡æ¡£ä¸­
            'other': 'docs/æ•™è‚²AIé¢˜åº“ç³»ç»Ÿæ–‡æ¡£.md'
        }

    def load_analysis_data(self) -> Tuple[Dict, Dict]:
        """åŠ è½½åˆ†ææ•°æ®"""
        inventory_file = os.path.join(self.repo_root, 'scripts', 'repo_inventory.json')
        analysis_file = os.path.join(self.repo_root, 'scripts', 'doc_analysis.json')
        
        with open(inventory_file, 'r', encoding='utf-8') as f:
            inventory = json.load(f)
        
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis = json.load(f)
        
        return inventory, analysis

    def generate_file_description(self, file_info: Dict) -> str:
        """ä¸ºæ–‡ä»¶ç”Ÿæˆä¸­æ–‡æè¿°"""
        category = file_info['category']
        file_type = file_info['type']
        file_path = file_info['path']
        
        # åŸºäºæ–‡ä»¶è·¯å¾„å’Œç±»å‹çš„æè¿°æ¨¡æ¿
        descriptions = {
            'backend': {
                'yaml': 'Kubernetesåç«¯æœåŠ¡éƒ¨ç½²é…ç½®æ–‡ä»¶ï¼Œå®šä¹‰äº†åç«¯åº”ç”¨çš„éƒ¨ç½²è§„èŒƒã€æœåŠ¡é…ç½®å’Œç½‘ç»œç­–ç•¥',
            },
            'frontend': {
                'yaml': 'Kuberneteså‰ç«¯æœåŠ¡éƒ¨ç½²é…ç½®æ–‡ä»¶ï¼Œå®šä¹‰äº†å‰ç«¯åº”ç”¨çš„éƒ¨ç½²è§„èŒƒå’Œè´Ÿè½½å‡è¡¡é…ç½®',
                'markdown': 'å‰ç«¯UIç³»ç»Ÿè®¾è®¡æ–‡æ¡£ï¼ŒåŒ…å«ç»„ä»¶è®¾è®¡ã€æ ·å¼è§„èŒƒå’Œäº¤äº’æµç¨‹'
            },
            'infrastructure': {
                'yaml': 'KubernetesåŸºç¡€è®¾æ–½é…ç½®æ–‡ä»¶ï¼Œç”¨äºéƒ¨ç½²å’Œç®¡ç†å®¹å™¨åŒ–åº”ç”¨çš„åŸºç¡€ç»„ä»¶',
                'python': 'åŸºç¡€è®¾æ–½ç®¡ç†å’Œè‡ªåŠ¨åŒ–è„šæœ¬ï¼Œç”¨äºéƒ¨ç½²ã€é…ç½®å’Œç»´æŠ¤ç³»ç»Ÿç»„ä»¶'
            },
            'scripts': {
                'python': 'é¡¹ç›®è‡ªåŠ¨åŒ–è„šæœ¬ï¼Œç”¨äºä»£ç ç”Ÿæˆã€æ–‡æ¡£åŒæ­¥ã€éƒ¨ç½²å’Œç»´æŠ¤ç­‰ä»»åŠ¡',
                'bash': 'ç³»ç»Ÿè¿ç»´Shellè„šæœ¬ï¼Œç”¨äºæœåŠ¡ç®¡ç†ã€æ•°æ®åº“æ“ä½œå’Œç³»ç»Ÿç»´æŠ¤',
                'sql': 'SQLæ•°æ®åº“è„šæœ¬ï¼Œç”¨äºæ•°æ®åº“ç»“æ„ä¼˜åŒ–ã€ç´¢å¼•åˆ›å»ºå’Œæ•°æ®ç»´æŠ¤',
                'markdown': 'è„šæœ¬ä½¿ç”¨è¯´æ˜å’Œè¿è¡ŒæŠ¥å‘Šæ–‡æ¡£'
            },
            'other': {
                'yaml': 'CI/CDå·¥ä½œæµé…ç½®æ–‡ä»¶ï¼Œå®šä¹‰äº†è‡ªåŠ¨åŒ–æ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²æµç¨‹',
                'markdown': 'é¡¹ç›®ç›¸å…³çš„è¯´æ˜æ–‡æ¡£å’ŒæŠ¥å‘Šæ–‡ä»¶'
            }
        }
        
        # åŸºäºå…·ä½“æ–‡ä»¶åçš„ç‰¹æ®Šæè¿°
        special_descriptions = {
            'repo_scanner.py': 'ä»“åº“æ–‡ä»¶æ‰«æå·¥å…·ï¼Œç”¨äºåˆ†æé¡¹ç›®ç»“æ„å’Œç”Ÿæˆæ–‡ä»¶æ¸…å•',
            'doc_parser.py': 'æ–‡æ¡£è§£æå·¥å…·ï¼Œç”¨äºåˆ†ææ–‡æ¡£ç»“æ„å’Œè¯†åˆ«æ–‡æ¡£åŒ–çŠ¶æ€',
            'auto_review_md_vs_code.py': 'æ–‡æ¡£ä»£ç ä¸€è‡´æ€§æ£€æŸ¥å·¥å…·ï¼Œå¯¹æ¯”æ–‡æ¡£ä¸­çš„ä»£ç å—ä¸å®é™…æ–‡ä»¶',
            'sync_md_code.py': 'æ–‡æ¡£ä»£ç åŒæ­¥å·¥å…·ï¼Œå°†æ–‡æ¡£ä¸­çš„ä»£ç å—åŒæ­¥åˆ°å®é™…æ–‡ä»¶',
            'rename_yaml_by_content.py': 'YAMLæ–‡ä»¶é‡å‘½åå·¥å…·ï¼Œæ ¹æ®æ–‡ä»¶å†…å®¹è‡ªåŠ¨åˆ†ç±»å’Œé‡å‘½å',
            'ingress.yaml': 'Kubernetes Ingressé…ç½®ï¼Œå®šä¹‰äº†å¤–éƒ¨è®¿é—®è·¯ç”±å’ŒSSLç»ˆæ­¢',
            'redis-deployment.yaml': 'Redisç¼“å­˜æœåŠ¡éƒ¨ç½²é…ç½®ï¼Œç”¨äºä¼šè¯å­˜å‚¨å’Œä»»åŠ¡é˜Ÿåˆ—',
            'mariadb-configmap.yaml': 'MariaDBæ•°æ®åº“é…ç½®æ˜ å°„ï¼ŒåŒ…å«æ•°æ®åº“åˆå§‹åŒ–å’Œé…ç½®å‚æ•°',
            'auto-review.yml': 'GitHub Actionsè‡ªåŠ¨å®¡æŸ¥å·¥ä½œæµï¼Œç”¨äºä»£ç è´¨é‡æ£€æŸ¥å’Œæ–‡æ¡£åŒæ­¥éªŒè¯'
        }
        
        # è·å–æ–‡ä»¶å
        filename = Path(file_path).name
        
        # æ£€æŸ¥ç‰¹æ®Šæè¿°
        if filename in special_descriptions:
            return special_descriptions[filename]
        
        # ä½¿ç”¨é€šç”¨æè¿°
        if category in descriptions and file_type in descriptions[category]:
            return descriptions[category][file_type]
        
        # é»˜è®¤æè¿°
        return f"{category}ç›®å½•ä¸‹çš„{file_type}æ–‡ä»¶ï¼Œ{filename}"

    def read_file_content(self, file_path: str) -> str:
        """å®‰å…¨è¯»å–æ–‡ä»¶å†…å®¹"""
        abs_path = os.path.join(self.repo_root, file_path)
        try:
            with open(abs_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content.strip()
        except UnicodeDecodeError:
            try:
                with open(abs_path, 'r', encoding='gb2312') as f:
                    content = f.read()
                return content.strip()
            except:
                return f"# æ–‡ä»¶ç¼–ç é—®é¢˜ï¼Œæ— æ³•è¯»å–: {file_path}"
        except Exception as e:
            return f"# è¯»å–æ–‡ä»¶å¤±è´¥: {e}"

    def get_language_from_extension(self, file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–ä»£ç å—è¯­è¨€"""
        ext_mapping = {
            '.py': 'python',
            '.js': 'javascript',
            '.vue': 'vue',
            '.yaml': 'yaml',
            '.yml': 'yaml',
            '.json': 'json',
            '.md': 'markdown',
            '.sql': 'sql',
            '.sh': 'bash',
            '.txt': 'plaintext',
            '.mmd': 'mermaid'
        }
        
        ext = Path(file_path).suffix.lower()
        return ext_mapping.get(ext, 'plaintext')

    def generate_documentation_section(self, file_info: Dict) -> str:
        """ä¸ºæ–‡ä»¶ç”Ÿæˆæ–‡æ¡£ç« èŠ‚"""
        file_path = file_info['path']
        description = self.generate_file_description(file_info)
        content = self.read_file_content(file_path)
        language = self.get_language_from_extension(file_path)
        
        # ç”Ÿæˆæ–‡æ¡£ç« èŠ‚
        section = f"""

#### {file_path}

{description}

```{language}
{content}
```
"""
        return section

    def find_insertion_point(self, doc_content: str, category: str) -> int:
        """æ‰¾åˆ°æ’å…¥æ–°ç« èŠ‚çš„ä½ç½®"""
        # ä¸ºä¸åŒç±»åˆ«çš„æ–‡ä»¶æ‰¾åˆ°åˆé€‚çš„æ’å…¥ä½ç½®
        insertion_patterns = {
            'infrastructure': [
                r'## 4\. éƒ¨ç½²æ–‡æ¡£',
                r'### 4\.2 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²',
                r'## éƒ¨ç½²',
                r'## k8s',
                r'## Kubernetes'
            ],
            'scripts': [
                r'## è„šæœ¬',
                r'## å·¥å…·',
                r'## è‡ªåŠ¨åŒ–',
                r'## ç»´æŠ¤',
                r'$'  # æ–‡æ¡£æœ«å°¾
            ],
            'backend': [
                r'## åç«¯',
                r'## backend',
                r'## æ ¸å¿ƒæ–‡ä»¶å®ç°',
                r'### 1\. åç«¯æ ¸å¿ƒæ–‡ä»¶'
            ],
            'frontend': [
                r'## å‰ç«¯',
                r'## frontend',
                r'## UI',
                r'## ç»„ä»¶'
            ],
            'other': [
                r'## å…¶ä»–',
                r'## é…ç½®',
                r'$'  # æ–‡æ¡£æœ«å°¾
            ]
        }
        
        patterns = insertion_patterns.get(category, [r'$'])
        
        for pattern in patterns:
            if pattern == '$':
                return len(doc_content)
            
            match = re.search(pattern, doc_content, re.MULTILINE)
            if match:
                # æ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜çš„ä½ç½®
                section_start = match.start()
                remaining_content = doc_content[section_start:]
                
                # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªåŒçº§æ ‡é¢˜
                next_section = re.search(r'\n## ', remaining_content)
                if next_section:
                    return section_start + next_section.start()
                else:
                    return len(doc_content)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ä½ç½®ï¼Œæ’å…¥åˆ°æœ«å°¾
        return len(doc_content)

    def add_section_header_if_needed(self, doc_content: str, category: str) -> str:
        """å¦‚æœéœ€è¦ï¼Œæ·»åŠ æ–°çš„ç« èŠ‚æ ‡é¢˜"""
        section_headers = {
            'infrastructure': '## åŸºç¡€è®¾æ–½é…ç½®æ–‡ä»¶',
            'scripts': '## è‡ªåŠ¨åŒ–è„šæœ¬å’Œå·¥å…·',
            'other': '## å…¶ä»–é…ç½®æ–‡ä»¶'
        }
        
        header = section_headers.get(category)
        if not header:
            return doc_content
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åº”çš„ç« èŠ‚
        if re.search(header.replace('##', r'##\s*'), doc_content):
            return doc_content
        
        # åœ¨åˆé€‚çš„ä½ç½®æ·»åŠ ç« èŠ‚æ ‡é¢˜
        insertion_point = self.find_insertion_point(doc_content, category)
        
        new_section = f"\n\n{header}\n"
        new_content = (doc_content[:insertion_point] + 
                      new_section + 
                      doc_content[insertion_point:])
        
        return new_content

    def update_documentation(self, undocumented_files: List[Dict]) -> Dict:
        """æ›´æ–°æ–‡æ¡£ï¼Œæ·»åŠ æœªæ–‡æ¡£åŒ–çš„æ–‡ä»¶"""
        updates = {}
        
        # æŒ‰æ–‡æ¡£åˆ†ç»„æ–‡ä»¶
        files_by_doc = {}
        for file_info in undocumented_files:
            category = file_info['category']
            target_doc = self.file_to_doc_mapping.get(category)
            
            if target_doc and target_doc in self.main_docs:
                if target_doc not in files_by_doc:
                    files_by_doc[target_doc] = []
                files_by_doc[target_doc].append(file_info)
        
        # æ›´æ–°æ¯ä¸ªæ–‡æ¡£
        for doc_path, files in files_by_doc.items():
            abs_doc_path = os.path.join(self.repo_root, doc_path)
            
            if not os.path.exists(abs_doc_path):
                print(f"âš ï¸ æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {doc_path}")
                continue
            
            # è¯»å–åŸå§‹æ–‡æ¡£
            with open(abs_doc_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            updated_content = original_content
            
            # æŒ‰ç±»åˆ«åˆ†ç»„æ–‡ä»¶
            files_by_category = {}
            for file_info in files:
                category = file_info['category']
                if category not in files_by_category:
                    files_by_category[category] = []
                files_by_category[category].append(file_info)
            
            # ä¸ºæ¯ä¸ªç±»åˆ«æ·»åŠ æ–‡ä»¶
            for category, category_files in files_by_category.items():
                # æ·»åŠ ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
                updated_content = self.add_section_header_if_needed(updated_content, category)
                
                # ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆæ–‡æ¡£ç« èŠ‚
                for file_info in category_files:
                    section = self.generate_documentation_section(file_info)
                    
                    # æ‰¾åˆ°æ’å…¥ä½ç½®
                    insertion_point = self.find_insertion_point(updated_content, category)
                    
                    # æ’å…¥æ–°ç« èŠ‚
                    updated_content = (updated_content[:insertion_point] + 
                                     section + 
                                     updated_content[insertion_point:])
            
            # ä¿å­˜æ›´æ–°çš„æ–‡æ¡£
            backup_path = abs_doc_path + f'.backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
            
            # åˆ›å»ºå¤‡ä»½
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(original_content)
            
            # ä¿å­˜æ›´æ–°
            with open(abs_doc_path, 'w', encoding='utf-8') as f:
                f.write(updated_content)
            
            updates[doc_path] = {
                'files_added': len(files),
                'backup_created': backup_path,
                'categories': list(files_by_category.keys())
            }
            
            print(f"âœ… å·²æ›´æ–°æ–‡æ¡£: {doc_path}")
            print(f"   æ·»åŠ æ–‡ä»¶: {len(files)} ä¸ª")
            print(f"   å¤‡ä»½åˆ›å»º: {backup_path}")
        
        return updates

def main():
    """ä¸»å‡½æ•°"""
    repo_root = '/home/runner/work/Edu-ai-question-bank/Edu-ai-question-bank'
    
    # åˆå§‹åŒ–å¼•æ“
    engine = AutoDocumentationEngine(repo_root)
    
    # åŠ è½½åˆ†ææ•°æ®
    try:
        inventory, analysis = engine.load_analysis_data()
    except FileNotFoundError as e:
        print(f"âŒ è¯·å…ˆè¿è¡Œ repo_scanner.py å’Œ doc_parser.py: {e}")
        return
    
    # è·å–æœªæ–‡æ¡£åŒ–çš„æ–‡ä»¶
    undocumented_files = []
    existing_files = {f['path']: f for f in inventory['files']}
    
    for file_path in analysis['comparison']['existing_but_undocumented']:
        if file_path in existing_files:
            undocumented_files.append(existing_files[file_path])
    
    # è¿‡æ»¤æ‰ä¸éœ€è¦è‡ªåŠ¨æ–‡æ¡£åŒ–çš„æ–‡ä»¶ï¼ˆå¦‚æ–‡æ¡£æ–‡ä»¶æœ¬èº«ï¼‰
    filtered_files = [
        f for f in undocumented_files 
        if f['category'] != 'docs' and not f['path'].endswith('.md')
    ]
    
    print(f"ğŸš€ å¼€å§‹è‡ªåŠ¨è¡¥å…¨æ–‡æ¡£...")
    print(f"ğŸ“Š å°†å¤„ç† {len(filtered_files)} ä¸ªæœªæ–‡æ¡£åŒ–çš„æ–‡ä»¶")
    
    # æŒ‰ç±»åˆ«æ˜¾ç¤ºæ–‡ä»¶
    categories = {}
    for f in filtered_files:
        cat = f['category']
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(f['path'])
    
    for category, files in categories.items():
        print(f"  [{category}] {len(files)} ä¸ªæ–‡ä»¶")
    
    # æ‰§è¡Œæ–‡æ¡£æ›´æ–°
    updates = engine.update_documentation(filtered_files)
    
    print(f"\nâœ… æ–‡æ¡£è‡ªåŠ¨è¡¥å…¨å®Œæˆ!")
    print(f"ğŸ“ æ›´æ–°äº† {len(updates)} ä¸ªæ–‡æ¡£æ–‡ä»¶:")
    
    for doc_path, info in updates.items():
        print(f"  {doc_path}: æ·»åŠ äº† {info['files_added']} ä¸ªæ–‡ä»¶")
    
    # ä¿å­˜æ›´æ–°æŠ¥å‘Š
    report_file = os.path.join(repo_root, 'scripts', 'auto_completion_report.json')
    report = {
        'timestamp': datetime.now().isoformat(),
        'processed_files': len(filtered_files),
        'updated_docs': updates,
        'files_by_category': categories
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ æ›´æ–°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    main()